import pandas as pd
from aiohttp import ClientSession, TCPConnector
from pypeln import asyncio_task as aio
from bs4 import BeautifulSoup
import json
import aiofiles




# Read Post Code File
post_c = pd.read_csv('../data/PC_TEST.csv', names=['postcode'])

# Convert Post Code to generator
post_i = iter(post_c['postcode'].tolist())

# Create Url generator using Post Code
urls = ("https://www.realestate.com.au/rent/in-nsw+{}/list-1?includeSurrounding=false".format(i) for i in post_i)

limit = 100
list_data = []
multi_pages = []
list_data = []


async def fetch(url, session):
    print("Scraping: " + url)
    async with session.get(url) as response:
        raw_data = await response.read()
        if response.status == 200:
            # print(raw_data)
            await parse(url, raw_data)
            

async def parse(url, html):
    if str(html) != "None":
        print("Parsing Html!"+url)
        soup = BeautifulSoup(html, "html.parser")
        listings=soup.select('.listingInfo')
        for listing in listings:
            price = listing.find(class_='priceText').get_text()
            address = listing.find(class_='rui-truncate').get_text()
            url = "https://www.realestate.com.au/" + listing.find(class_='rui-truncate').find(class_='name').get('href')
            details = listing.find(class_='rui-property-features').get_text()
            new_data = {"Address": address, "details": details, "price": price,"url": url}
            list_data.append(new_data)


aio.each(
    fetch,
    urls,
    workers=limit,
    on_start=lambda: ClientSession(connector=TCPConnector(limit=None, verify_ssl=False)),
    on_done=lambda _status, session: session.close(),
    run=True,
)